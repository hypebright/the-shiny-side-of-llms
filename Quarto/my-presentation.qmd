---
title: "The Shiny Side of LLMs"
author: "Veerle Eeftink - van Leemput"
format:
  revealjs: 
    theme: [default]
include-in-header: 
  text: |
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
highlight-style: "nord"
---

# Hey ðŸ‘‹

In this demo presentation we will explore how to use LLMs in Shiny applications. Because everybody is doing it, right? ðŸ˜‰

# What do you need?

- The means to authenticate. Depending on your provider, you might need an API key (e.g. Anthropic, OpenAI), a token (e.g. Hugging Face), or other authentication methods.
- Put those credentials in a suitable place (for R: .Renviron, for Python: .env)
- Install the necessary packages:
  - For R: `shiny`, `ellmer`
  - For Python: `shiny`, `chatlas`
- An creative idea for your LLM-powered app!

# Using ellmer

Some R code:
```r
library(ellmer)

chat <- chat_anthropic(
  model = "claude-sonnet-4",
  system_prompt = "You are a friendly but terse assistant.",
)

chat$chat("Teach me some Shiny for R")
```

# Using chatlas

Some Python code:
```python
import chatlas as ctl

chat = ctl.ChatAnthropic(
    model = "claude-sonnet-4",
    system_prompt = "You are a friendly but terse assistant.",
)

chat.chat("Teach me some Shiny for Python")
```

# Building an app with Shiny

- Shiny is a web application framework for R and Python that allows you to build interactive web applications.
- It provides a reactive programming model, making it easy to create dynamic user interfaces and handle user inputs.
- Perfect for your AI-powered app!

# Using Shiny for R

```r
library(shiny)
library(bslib)

ui <- page_fluid(
  theme = bs_theme(bootswatch = "minty"),
  
  # App title
  h1("Simple Shiny demo"),
  
  # Create a card
  card(
    card_header("Welcome"),
    p("This is a simple Shiny app using bslib for layout and styling."),

    # Input: Slider
    textAreaInput("text", "What's your question?"),
    
    # Output: echo the text back
    textOutput("echo")
  )
)

server <- function(input, output, session) {
  # Reactive output
  output$echo <- renderText({
    paste("You asked:", input$text)
  })
}

shinyApp(ui, server)
```

# Using Shiny for Python

```python
from shiny import App, ui, render

# Define UI
app_ui = ui.page_fluid(
    ui.h1("Simple Shiny App"),
    
    # Card with slider
    ui.card(
        ui.card_header("Welcome"),
        
        ui.p("This is a basic Shiny for Python application."),
        
        # Input: Slider
        ui.input_text_area("text", "What's your question?"),
        
        # Output: echo text back
        ui.output_text("echo")
    )
)

# Define server
def server(input, output, session):
    @render.text
    def echo():
        return f"You asked: {input.text()}"

# Create app
app = App(app_ui, server)
```

# Where's the magic?

We're here for some AI magic, right? So let's talk to an LLM via our Shiny application!

![AI Magic](https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExMmZ0cmd1aGZzM3JqdW1zbng0czU2c2RwejNtM25jMW8xdnYzNXVrdCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/OlxeZT285uhFydNetO/giphy.gif)

# Shiny and chatlas
```python

```

# Shiny and ellmer
```r

```

# User experience

When using an LLM in a Shiny application, it is important to consider the user experience. The LLM should enhance the application, not overwhelm it. This means that theLLMI responses should be concise and relevant to the user's query. Additionally, it is crucial to handle errors gracefully, providing users with helpful feedback if the LLM cannot generate a response or if there are issues with the API. Therefore, implementing proper error handling and fallback mechanisms is essential.
